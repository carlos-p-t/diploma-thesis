\section{Object Detector}\label{sec:objectdetector}

The object detector training is divided into two parts: base training and fine-tuning. We refer to the initial training of the model on the largest dataset as "base training" and the subsequent training on the smaller dataset as "fine-tuning." Datasets are neither merged nor combined in any way. All training experiments began with a pre-trained YOLO model. This pre-trained YOLO model (medium or large size) was initially trained exclusively on the Mapillary Vistas subset using the default hyperparameters, after which different hyperparameters were adjusted to observe their impact on the results. Both YOLOv8 and YOLO11 were the pre-trained models selected for these experiments. The first was chosen for its robustness in object detection tasks, while the second was selected because it is the most recent model in the pre-trained series; thus, both were chosen to determine whether they generate different results after training. Table 3.1 displays the most important default hyperparameters that were later altered for experimentation. \\

Once a base training was complete, the trained model was tested on the Mapillary Vistas testing subset. From all base training experiments, two of the best models were selected: one based on YOLOv8 and one based on YOLO11. To choose these models, five metrics were taken into account: Precision, Recall, mAP@50, mAP@50-95, and Fitness. However, since our research focuses on evaluating detection quality and its connection to driver attention, mAP@50 and Recall were the most relevant metrics for determining which model performed the best.\\

\begin{table}[!h]
\centering
\renewcommand{\arraystretch}{1.3} % Adjust row height
\begin{tabular}{|c|c|}
\hline
\textbf{Hyperparameter} & \textbf{Default Value} \\ \hline
\textbf{Training Epochs} & 50 \\ \hline
\textbf{Batch Size} & 16  \\ \hline
\textbf{Image Size} & 640  \\ \hline
\textbf{Optimizer} & Stochastic Gradient Descent (SGD)  \\ \hline
\textbf{Cosine Learning Rate Scheduler} & False \\ \hline
\textbf{Initial Learning Rate} & 0.01 \\ \hline
\textbf{Final Learning Rate} & 0.01 \\ \hline
\textbf{Momentum} & 0.937 \\ \hline
\textbf{Weight Decay} & 0.00005 \\ \hline
\textbf{Data Augmentation: Rotation} & 0.0 \\ \hline
\textbf{Data Augmentation: Scaling} & 0.5 \\ \hline
\textbf{Data Augmentation: Flipping Upside Down} & 0.0 \\ \hline
\textbf{Data Augmentation: Flipping Left to Right} & 0.5 \\ \hline
\end{tabular}
\caption{YOLO pre-trained models default hyperparameters that were changed through different training experiments.}
\label{tab:defaulthyperparameters}
\end{table}

After selecting the two best base models, they were fine-tuned using the BillboardLamac Dataset. Similarly, after experimenting with different combinations of hyperparameters and testing each model on the BillboardLamac Dataset testing subset, two final models were selected based on their metric results.

